<style>
    .image-container {
        position: relative;
        display: inline-block;
    }
    .label {
        position: absolute;
        top: 4px;
        left: 4px;
        background-color: rgba(0, 0, 255, 0.7); /* ËìùËâ≤ËÉåÊôØÔºåÈÄèÊòéÂ∫¶‰∏∫0.7 */
        color: white;
        padding: 4px 8px;
        font-size: 12px;
        border-radius: 4px;
    }
    img {
        display: block;
    }
</style>


# üìù Publications 
<!-- Âä†ÁÇπË°®ÊÉÖÂåÖ,Áõ¥Êé•Â§çÂà∂ÂõæÁâáÂç≥ÂèØ  https://github.com/guodongxiaren/README/blob/master/emoji.md?tdsourcetag=s_pcqq_aiomsg -->

A full publication list is available on my [google scholar](https://scholar.google.com/citations?user=yuiXa5EAAAAJ&hl=en&oi=ao) page.

(*: equal contribution; ‚Ä†: corresponding authors.)


<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/videogen.jpg" width="350"> <div class="label">Video Generation</div></div>
</th><th style="text-align:left" width="70%"> <span style="font-size:18px">A Survey on Video Diffusion Models</span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">  ACM Computing Survey (<strong>CSUR, IF=23.8</strong>), 2024</span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2310.10647">Paper</a>][<a href="https://github.com/ChenHsing/Awesome-Video-Diffusion-Models">HomePage</a>][<a href="https://zhuanlan.zhihu.com/p/661860981">Zhihu</a>][<a href="https://mp.weixin.qq.com/s/qes6C8UbEYArnVKU3eu9QQ">Êú∫Âô®‰πãÂøÉ</a>][<a href="https://mp.weixin.qq.com/s/viC_J08bVIVRzvRYxRyQTw">ÈáèÂ≠ê‰Ωç</a>]</span><br> <span style="color: red;"> Surveying 300+ recent literatures on video generation and editing with diffusion models. Acheving Github 2000+ stars.
</span>
</th>
</tr></tbody></table>


<table style="width:100%">
<tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/simda.jpg" width="350"> <div class="label">Video Generation</div></div>
</th><th style="text-align:left" width="70%"> 
<span style="font-size:18px">SimDA: A Simple Diffusion Adapter for Efficient Video Generation</span><br> 
<span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qi Dai, Han Hu, Zuxuan Wu, Yu-Gang Jiang</span></span><br>
 <span style="font-weight:normal;font-size:16px">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024 </span><br> 
 <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2308.09710">Paper</a>][<a href="https://chenhsing.github.io/SimDA/">HomePage</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/stableanimator.jpg" width="350"> <div class="label">Video Generation</div></div>
</th><th style="text-align:left" width="70%"> <span style="font-size:18px">StableAnimator: High-Quality Identity-Preserving Human Image Animation
</span><br> <span style="font-weight:normal;font-size:16px">Shuyuan Tu, </span> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, Zuxuan Wu</span></span><br> <span style="font-weight:normal;font-size:16px"> IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025</span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2411.17697">Paper</a>][<a href="https://github.com/Francis-Rings/StableAnimator">Code</a>][<a href="https://francis-rings.github.io/StableAnimator/">Homepage</a>][<a href="https://mp.weixin.qq.com/s/qK3s-us2XeDv7phW83W5BQ">Êú∫Âô®‰πãÂøÉ</a>]</span><br> <span style="color: red;"> Acheving Github 1200+ stars.
</span>
</th>
</tr></tbody></table>


<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/magicmotion.jpg" width="350"> <div class="label">Video Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance </span><br> <span style="font-weight:normal;font-size:16px">Quanhao Li*</span>, <span style="font-size:16px">Zhen Xing*<span style="font-weight:normal">, Rui Wang, Hui Zhang, Zuxuan Wu</span></span><br> <span style="font-weight:normal;font-size:16px">Technical Report, 2025 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/pdf/2503.16421">Paper</a>][<a href="https://github.com/quanhaol/MagicMotion/">Code</a>][<a href="https://quanhaol.github.io/magicmotion-site/">HomePage</a>][<a href="https://mp.weixin.qq.com/s/oGI4NIkVv9xV-pC19LLc3g">ÈáèÂ≠ê‰Ωç</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/genrec.png" width="350"> <div class="label">Video Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">GenRec: Unifying Video Generation and Recognition with Diffusion Models </span><br> <span style="font-weight:normal;font-size:16px">Zejia Weng, Xitong Yang</span>, <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2024 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2408.15241">Paper</a>]
</span></th></tr></tbody></table>



<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/aid.png" width="350"> <div class="label">Video Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">AID: Adapting Image2Video Diffusion Models for Instruction-based Video Prediction</span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qi Dai, Zejia Weng, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">Technical Report, 2024 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2406.06465">Paper</a>][<a href="https://chenhsing.github.io/AID/">HomePage</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/vidiff.jpg" width="350"> <div class="label">Video Editing</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models</span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qi Dai, Zihao Zhang, Hui Zhang, Han Hu, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">Technical Report, 2024 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2311.18837">Paper</a>][<a href="https://chenhsing.github.io/VIDiff/">HomePage</a>][<a href="https://zhuanlan.zhihu.com/p/670615911">Zhihu</a>]
</span></th></tr></tbody></table>


<table style="width:100%"><tbody><tr><th width="30%"><div class="image-container"> <img src="../../images/svformer.jpg" width="350"> <div class="label">Video Recongnition</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">SVFormer: Semi-supervised Video Transformer for Action Recognition </span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Qi Dai, Han Hu, Jingjing Chen, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2211.13222">Paper</a>][<a href="https://github.com/ChenHsing/SVFormer">Code</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/panoswin.png" width="350"> <div class="label">3D Understanding</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">PanoSwin: a Pano-style Swin Transformer for Panorama Understanding </span><br> <span style="font-weight:normal;font-size:16px">Zhixin Ling</span>, <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Manliang Cao, Xiangdong Zhou</span></span><br> <span style="font-weight:normal;font-size:16px">IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_PanoSwin_A_Pano-Style_Swin_Transformer_for_Panorama_Understanding_CVPR_2023_paper.pdf">Paper</a>][<a href="https://github.com/1069066484/PanoSwinTransformerObjectDetection">Code</a>]
</span></th></tr></tbody></table>


<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/ssp3d.jpg" width="350"> <div class="label">3D Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">Semi-supervised Single-view 3D Reconstruction via Prototype Shape Priors </span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Hengduo Li, Zuxuan Wu, Yu-Gang Jiang</span></span><br> <span style="font-weight:normal;font-size:16px">European Conference on Computer Vision (<strong>ECCV</strong>), 2022 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2209.15383">Paper</a>][<a href="https://github.com/ChenHsing/SSP3D">Code</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/mpcn.jpg" width="350"> <div class="label">3D Generation</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">Few-shot Single-view 3D Reconstruction with Memory Prior Contrastive Network </span><br> <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">, Yijiang Chen, Zhixin Ling, Xiangdong Zhou, Yu Xiang</span></span><br> <span style="font-weight:normal;font-size:16px">European Conference on Computer Vision (<strong>ECCV</strong>), 2022 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://arxiv.org/abs/2208.00183">Paper</a>][<a href="#">Code</a>]
</span></th></tr></tbody></table>

<table style="width:100%"><tbody><tr><th width="30%"> <div class="image-container"> <img src="../../images/csr.jpg" width="350"> <div class="label">Image Retrieval</div></div></th><th style="text-align:left" width="70%"> <span style="font-size:18px">Conditional Stroke Recovery for Fine-Grained Sketch-Based Image Retrieval </span><br> <span style="font-weight:normal;font-size:16px">Zhixin Ling</span>, <span style="font-size:16px">Zhen Xing<span style="font-weight:normal">,Jian Zhou, Xiangdong Zhou</span></span><br> <span style="font-weight:normal;font-size:16px">European Conference on Computer Vision (<strong>ECCV</strong>), 2022 </span><br> <span style="font-weight:normal;font-size:16px">[<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860708.pdf">Paper</a>][<a href="https://github.com/1069066484/CSR-ECCV2022">Code</a>]
</span></th></tr></tbody></table>


- ‚Äã**‚ÄãAdaDiff: Adaptive Step Selection for Fast Diffusion‚Äã**‚Äã  
  Hui Zhang, Zuxuan Wu, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Jie Shao, Yu-Gang Jiang  
  ‚Äã**‚ÄãAAAI‚Äã**‚Äã, 2025, [[Paper](https://arxiv.org/abs/2311.14768)]

- ‚Äã**‚ÄãAdvancing Dark Action Recognition via Modality Fusion and Dark-to-Light Diffusion Model‚Äã**‚Äã  
  Yuxuan Wang, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Zuxuan Wu  
  ‚Äã**‚ÄãICASSP‚Äã**‚Äã, 2025, [[Paper](https://ieeexplore.ieee.org/abstract/document/10890723)]

- ‚Äã**‚ÄãHuman2Robot: Learning Robot Actions from Paired Human-Robot Videos‚Äã**‚Äã  
  Sicheng Xie, Haidong Cao, Zejia Weng, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Shiwei Shen, Jiaqi Leng, Xipeng Qiu, Yanwei Fu, Zuxuan Wu, Yu-Gang Jiang  
  ‚Äã**‚ÄãArxiv‚Äã**‚Äã, 2025, [[Paper](https://arxiv.org/pdf/2502.16587)]

- ‚Äã**‚ÄãAligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms‚Äã**‚Äã  
  Miaosen Zhang, Yixuan Wei, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Yifei Ma, Zuxuan Wu, Ji Li, Zheng Zhang, Qi Dai, Chong Luo, Xin Geng, Baining Guo  
  ‚Äã**‚ÄãNeurIPS‚Äã**‚Äã, 2024, [[Paper](https://arxiv.org/pdf/2406.06465.pdf)], [[HomePage](https://chenhsing.github.io/AID/)]

- ‚Äã**‚ÄãFDGaussian: Fast Gaussian Splatting via Geometric-aware Diffusion Model‚Äã**‚Äã  
  Qijun Feng, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Zuxuan Wu, Yu-Gang Jiang  
  ‚Äã**‚ÄãArxiv‚Äã**‚Äã, 2024, [[Paper](https://arxiv.org/pdf/2403.10242.pdf)], [[HomePage](https://qjfeng.net/FDGaussian/)]

- ‚Äã**‚ÄãTranSFormer: Slow-Fast Transformer for Machine Translation‚Äã**‚Äã  
  Bei Li, Yi Jing, Xu Tan, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Tong Xiao, Jingbo Zhu  
  ‚Äã**‚ÄãACL (Findings)‚Äã**‚Äã, 2023, [[Paper](https://arxiv.org/pdf/2305.16982.pdf)]

- ‚Äã**‚ÄãMulti-Level Region Matching for Fine-Grained Sketch-Based Image Retrieval‚Äã**‚Äã  
  Zhixin Ling, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Jiangtong Li, Li Niu  
  ‚Äã**‚ÄãACM MM‚Äã**‚Äã, 2022, [[Paper](https://www.jiangtongli.me/publication/mlmr/mlmr.pdf)]

- ‚Äã**‚Äã3D-Augmented Contrastive Knowledge Distillation for Image-based Object Pose Estimation‚Äã**‚Äã  
  Zhidan Liu, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Xiangdong Zhou, Yijiang Chen, Guichun Zhou  
  ‚Äã**‚ÄãICMR‚Äã**‚Äã, 2022, [[Paper](https://arxiv.org/pdf/2206.02531.pdf)]

- ‚Äã**‚ÄãCaSS: A Channel-aware Self-supervised Representation Learning Framework for Multivariate Time Series Classification‚Äã**‚Äã  
  Yijiang Chen, Xiangdong Zhou, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Zhidan Liu, Minyang Xu  
  ‚Äã**‚ÄãDASFFA‚Äã**‚Äã, 2022, [[Paper](https://arxiv.org/pdf/2203.04298.pdf)]

- ‚Äã**‚ÄãFrom Coarse to Fine: Hierarchical Structure-aware Video Summarization‚Äã**‚Äã  
  Wenxu Li, Gang Pan, Chen Wang, ‚Äã**‚ÄãZhen Xing‚Äã**‚Äã, Zhenjun Han  
  ‚Äã**‚ÄãTOMM‚Äã**‚Äã, 2022, [[Paper](https://dl.acm.org/doi/abs/10.1145/3485472)]